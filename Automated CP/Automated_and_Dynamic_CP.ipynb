{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design and implementation of  automated and dynamic clinical pathways for low resource settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geletaw Sahle, Demesewu Amenu, Girum Ketema, Frank Verbeke, Jan Cornelis, Bart Jansen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal\n",
    "\n",
    "The aim of the research design and implementation is to:\n",
    "\n",
    "1.  Design a hybrid and dynamic algorithm for generating CP\n",
    "2.  Dynamically validating the knowledge-based CP using evidence (data-driven) i.e. dynamically validate and map the knowledge based clinical pathways with local conditions or context or tracing the history\n",
    "3.  Arrange (re-arrange) the decision priority of the CP based the context such as introducing multi criteria decision analysis (probabilistic based, severity based )\n",
    "4. Investigate a mechanism for potential multi-disease CPs generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas\n",
    "import pandas as pd \n",
    "\n",
    "# knowledge based indicators: Extracted from the CGs and used for a gold standard\n",
    "import import_ipynb\n",
    "import CG_rulesets_and_indicators\n",
    "\n",
    "#imports secure module for creating a secure random object\n",
    "import secrets      \n",
    "\n",
    "#Import pickle Package, to reterive the saved CP model\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "\n",
    "import os.path\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: Description of inputs, outputs and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input - List of the presented signs and symptoms\n",
    "\"\"\"\n",
    "Measured_Signs_and_Symptoms_List ={} \n",
    "\n",
    "\"\"\"\n",
    "    Potential list of signs and symptoms hasextracted from the CGs (Endorsed by WHO, \n",
    "    Ministry of Health and a group of experts)\n",
    "\"\"\"\n",
    "Potential_List_of_Signs_and_Symptoms = {}\n",
    "\n",
    "\"\"\"\n",
    "     An indicator (or criteria) extracted from the CGs (Expert Opinion). \n",
    "     Its used to check whether the measured symptoms satisfy the condition or not. \n",
    "     Also, used as an EXIT criteria\n",
    "\"\"\"\n",
    "Indicator = {}\n",
    "\n",
    "\"\"\"\n",
    "    Output -  A list of generated CP\n",
    "\"\"\"\n",
    "Generated_CP_LIST = {}  \n",
    "\n",
    "\"\"\"  A list of parameters for ranking CP \"\"\"\n",
    "RankingParameters = {'Probability', 'Severity', 'Cost', 'Weight', 'Evidence'} \n",
    "\n",
    "\"\"\" A list of pruning parameters \"\"\"\n",
    "PruningParameters = {'Probability', 'Severity', 'Cost', 'Weight', 'Evidence'} \n",
    "\n",
    "Flag = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indicator (or Exit Criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extracted the indicator (or Exit critieria) from the Clinical Guideline (CGs) used as a Gold Standard - through indvidual measured symptoms and combinations of measured symptoms. The indicator (or automated) is arranged in python nested dictionray structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1. Importing the knowleged based CP - CG rulesets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    The CG ruleset are structured in the form of nested dictionary strucutre \n",
    "\"\"\"\n",
    "import import_ipynb\n",
    "import CG_rulesets_and_indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Assign the imported indicator to a python dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign the imported indicator to a python dictionary \n",
    "indicator = CG_rulesets_and_indicators.CG_rulesets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3. Checking whether the Sample Indicators or Rulesets is proprely displayed or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator['referralIndicator']['urgentAttention']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to access the Knowledge Based CP rulesets (or indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class clinicalPathways_Indicator:\n",
    "    \"\"\"\n",
    "        A function for acessing and signaling urgent attention pathways\n",
    "    \"\"\"\n",
    "    def get_urgentAttention_indicators(*args):\n",
    "        urgentAttention_indicator = indicator['referralIndicator']['urgentAttention']\n",
    "        return urgentAttention_indicator\n",
    "\n",
    "    \"\"\"\n",
    "    A Function for acessing the referral Clinical Pathways indicators\n",
    "    \"\"\"\n",
    "    def get_Convulsion_R_indicator(*args):\n",
    "        convulision_indicator = indicator['referralIndicator']['convulsion']\n",
    "        return convulision_indicator\n",
    "    def get_severe_Pre_eclampsia_R_indicator(*args):\n",
    "        pre_eclampsia_indicator= indicator['referralIndicator']['severe_pre_eclampsia']\n",
    "        return pre_eclampsia_indicator\n",
    "    \n",
    "    def get_severe_hypertension_R_indicator(*args):\n",
    "        severe_hypertension_indicator= indicator['referralIndicator']['severe_hypertension']\n",
    "        return severe_hypertension_indicator\n",
    "    \n",
    "    def get_vaginalBleeding_R_indicator(*args):\n",
    "        vaginalBleeding_indicator= indicator['referralIndicator']['vaginalBleeding']\n",
    "        return vaginalBleeding_indicator\n",
    "    \n",
    "    def get_pretermLabour_R_indicator(*args):\n",
    "        pretermLabour_indicator= indicator['referralIndicator']['pretermLabour']\n",
    "        return pretermLabour_indicator\n",
    "    \n",
    "    def get_PROM_R_indicator(*args):\n",
    "        PROM_indicator= indicator['referralIndicator']['PROM']\n",
    "        return PROM_indicator\n",
    "    \n",
    "    def get_unsurePregnancy_R_indicator(*args):\n",
    "        unsurePregnancy_indicator= indicator['referralIndicator']['unsurePregnancy']\n",
    "        return unsurePregnancy_indicator\n",
    "\n",
    "    \"\"\"\n",
    "    A Function for acessing the treatable CLinical Pathways indicators\n",
    "    \"\"\"\n",
    "    def get_Convulsion_T_indicator(*args):\n",
    "        convulision_indicator = indicator['treatableIndicator']['convulsion']\n",
    "        return convulision_indicator\n",
    "    \n",
    "    def get_vaginalBleeding_T_indicator(*args):\n",
    "        vaginalBleeding_indicator = indicator['treatableIndicator']['vaginalBleeding']\n",
    "        return vaginalBleeding_indicator\n",
    "    \n",
    "    def get_pretermLabour_T_indicator(*args):\n",
    "        pretermLabour_indicator = indicator['treatableIndicator']['pretermLabour']\n",
    "        return pretermLabour_indicator\n",
    "    \n",
    "    def get_notUrgentAttention_T_indicator(*args):\n",
    "        notUrgentAttention_indicator = indicator['treatableIndicator']['notUrgentAttention']\n",
    "        return notUrgentAttention_indicator\n",
    "    \n",
    "    def get_unsurePregnancy_T_indicator(*args):\n",
    "        unsurePregnancy_indicator = indicator['treatableIndicator']['unsurePregnancy']\n",
    "        return unsurePregnancy_indicator\n",
    "    \n",
    "    \"\"\"\n",
    "    A Function for acessing the consideration Clinical Pathway indicators\n",
    "    \"\"\"\n",
    "    def get_unsurePregnancy_C_indicator(*args):\n",
    "        unsurePregnancy_indicator = indicator['considerationIndicator']['unsurePregnancy']\n",
    "        return unsurePregnancy_indicator\n",
    "    \n",
    "    def get_notUrgentAttention_C_indicator(*args):\n",
    "        notUrgentAttention_indicator = indicator['considerationIndicator']['notUrgentAttention']\n",
    "        return notUrgentAttention_indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPIndicator = clinicalPathways_Indicator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPIndicator.get_severe_Pre_eclampsia_R_indicator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Presented (or Measured) Symptoms: Input wizard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the dominat factor: Extracted from the CGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the dominat factor \n",
    "# the pregnant patient dominat factor based on the clinical guidelines \n",
    "dominat_Factors = dict([\n",
    "    ('convulsion', 'yes'),\n",
    "    ('diffcultyBreathing', 'yes'),     \n",
    "    ('headache', 'vaginalBleeding'),\n",
    "    ('vaginalBleeding', 'yes'),\n",
    "    ('abdominalPain', 'yes'),\n",
    "    ('swollenPainfulCalf', 'yes'),\n",
    "    ('decreased_absent_FetalMovements', 'yes'),\n",
    "    ('BP', ('≥140/90','≥160/110')),\n",
    "    ('blurredVision', 'yes'),\n",
    "    ('painfulContractions', 'yes'),\n",
    "    ('Sudden_GushOFclear_or_pale_fluid', 'yes'),\n",
    "    ('Temperature', '≥38°C'),\n",
    "    ('headache', 'yes'),\n",
    "    ('weakness', 'yes'),\n",
    "    ('backPain', 'yes'),\n",
    "    ('proteinuria', ('without proteinuria','≥ 1+ proteinuria')),\n",
    " ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entry Point Initiation: based on  dominat factor  and random choice (signs and symptoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import secrets                              # imports secure module.\n",
    "cp_EntrySigns = secrets.SystemRandom()      # creates a secure random object.\n",
    "presented_Signs_and_Symptoms = [] # initializing the presented signs and symptoms \n",
    "# Check the availablity of domainat factors be initiating the CP Entry\n",
    "for signs, value in dominat_Factors.items():\n",
    "    if \"yes\" in value:\n",
    "        #print(signs, value) # (signs, value)\n",
    "        presented_Signs_and_Symptoms.append(signs) # add the dominant presented signs into the presented signs and symptoms list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the CP Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the CP Entry \n",
    "entryNum_to_select = 4               # set the number to select here (just convention).\n",
    "list_of_presented_signs = cp_EntrySigns.sample(presented_Signs_and_Symptoms, entryNum_to_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A CP wizard function for accepting inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CP_Input_Wizard():\n",
    "    print(\"Enter the presented signs and symptoms?\")\n",
    "    response = ''\n",
    "    for Symptoms in list_of_presented_signs:\n",
    "        # accepting signs and symptoms randomly\n",
    "        user_input = input(Symptoms)\n",
    "        responseList = user_input.split()\n",
    "        \n",
    "        # validate the measured sysmptoms \n",
    "        if user_input == \"yes\":\n",
    "            Measured_Signs_and_Symptoms_List[Symptoms] = responseList\n",
    "        # control and validate the user inputs \n",
    "        if Symptoms == 'Temprature' and response.lower() not in {\">=38\", \"<38\"}:\n",
    "            response = input(\"Please enter >=38 or <38:\")\n",
    "        elif Symptoms == 'BP' and response.lower() not in {\">=160/90\", \"<160/90\"}:\n",
    "            response = input(\"Please enter '>=160/90' or '<160/90':\")\n",
    "        elif Symptoms == 'Fever' and response.lower() not in {\"yes\", \"no\",\"persitant\"}:\n",
    "            response = input(\"Please enter yes, no or persitant:\")\n",
    "        else:\n",
    "            while response.lower() not in {\"yes\", \"no\"}:                \n",
    "                response = input(\"Please enter yes or no: \")\n",
    "        # if the measured sysmptoms fullfill add to measured symptoms list\n",
    "\n",
    "            #res.append(Symptoms)\n",
    "    return Measured_Signs_and_Symptoms_List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call to accept the measured symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Measured_Signs_and_Symptoms_List = CP_Input_Wizard() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Measured_Signs_and_Symptoms_List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Explore Possible Measured Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from itertools import combinations\n",
    "\n",
    "Measured_Signs_and_Symptoms_List = {\n",
    "    'convulsion': 'yes', \n",
    "    'Fever' : 'yes',             \n",
    "    'BP':'>=140/90', \n",
    "    'headache':'yes',\n",
    "    'blurredVision':'yes', \n",
    "    'abdominalPain':'yes', \n",
    "    'bleeding':'yes',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Measured_Signs_and_Symptoms_List['BP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Possible Combinations for generating CP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        A function to generate possible combinations of signs and symptoms: the function will return both indvidual \n",
    "        measured sysmptoms and possible combinations of measrued Symptoms\n",
    "\"\"\"\n",
    "Possiblecombinations = []\n",
    "nestedList = {}\n",
    "def possible_SignsandSymptoms_Combinations(value):\n",
    "    noOFmeasuredSymptoms = len(value)\n",
    "    for j in range(1, noOFmeasuredSymptoms+1):\n",
    "        comb = combinations(value, j) \n",
    "        for i in list(comb): \n",
    "            Possiblecombinations.append(i)\n",
    "    return Possiblecombinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nestedList = possible_SignsandSymptoms_Combinations(Measured_Signs_and_Symptoms_List)\n",
    "#nestedList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking and Validating Combination of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    iterate over the generated combination of measured parameters nested list for indexing and validating \n",
    "    the value of each combination of measured parameters for CP generation\n",
    "\"\"\"\n",
    "possibleComb = {} \n",
    "def comibnationofParam(nestedList):\n",
    "    # iterate the generated combination of possible parameters and generate CP\n",
    "    i=0 #used for indexing the dictionary\n",
    "    for subList in nestedList:\n",
    "        res = {i: {k: Measured_Signs_and_Symptoms_List['BP'] if k =='BP' else 'yes' for k in subList}}\n",
    "        possibleComb.update(res)\n",
    "        i=i+1\n",
    "    return possibleComb #return possible combination in Dictionary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 7 measured symptoms found , incrementaly it will explore 2(7) possible combinations.. \n",
    "# presented posible combination of measured symptoms i.e.\n",
    "presentedMeasuredSymptoms = comibnationofParam(nestedList)\n",
    "#res # it will return index pythond dictionray list of measured symptoms (both individual and combinatio of measured symptoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3. Potential eligble measured symptoms for CP generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presentedMeasuredSymptoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process (or Generate) Clinical Pathways "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intialize the generated CP Output List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize the generated CP list\n",
    "Generated_CP_LIST = pd.DataFrame({'Measured_Symptoms':[],'Urgent_Attention':'','Generated_CP':[],'Finding':[],'Evidence':[],'Prior_Prob':[],'Accuracy':[],'Pred_CP':[], 'Severity':[], 'Cost':[], 'Weight':[]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the CG indicators (rulesets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call all the knowledge based indicators\n",
    "indicators = {\n",
    "    'urgentIndicators':{0:clinicalPathways_Indicator.get_urgentAttention_indicators()},\n",
    "    'referralIndicators':{\n",
    "        0:clinicalPathways_Indicator.get_Convulsion_R_indicator(),\n",
    "        1:clinicalPathways_Indicator.get_severe_Pre_eclampsia_R_indicator(),\n",
    "        2:clinicalPathways_Indicator.get_severe_hypertension_R_indicator(),\n",
    "        3:clinicalPathways_Indicator.get_vaginalBleeding_R_indicator(),\n",
    "        4:clinicalPathways_Indicator.get_pretermLabour_R_indicator(),\n",
    "        5:clinicalPathways_Indicator.get_PROM_R_indicator(),\n",
    "        6:clinicalPathways_Indicator.get_unsurePregnancy_R_indicator(),\n",
    "    },\n",
    "    'treatableIndicators':{\n",
    "        0:clinicalPathways_Indicator.get_Convulsion_T_indicator(),\n",
    "        1:clinicalPathways_Indicator.get_vaginalBleeding_T_indicator(),\n",
    "        2:clinicalPathways_Indicator.get_pretermLabour_T_indicator(),\n",
    "        #3:clinicalPathways_Indicator.get_notUrgentAttention_T_indicator(),\n",
    "        3:clinicalPathways_Indicator.get_unsurePregnancy_T_indicator(),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mss  = {\n",
    "    0:{\"convulsion\":\"yes\"},\n",
    "    1:{\"Fever\":\"Yes\"},\n",
    "    2:{\"BP\":\">=140/90\"},\n",
    "    3:{\"temprature\":\">=38\"}\n",
    "}\n",
    "len(mss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in mss:\n",
    "    print(mss[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(indicators['urgentIndicators'][0])):\n",
    "    print(indicators['urgentIndicators'][0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in mss:\n",
    "    for i in range(0, len(indicators['urgentIndicators'][0])):\n",
    "        if mss[col] == indicators['urgentIndicators'][0][i]:\n",
    "            print(mss[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Function for generating Clinical Pathways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clinicalPathway_generator(Generated_CP_LIST,presentedMeasuredSymptoms, indicators, Urgent_Attention, CP, finding):\n",
    "    \"\"\"\n",
    "        Generate the clinical pathway based on the measured symptoms\n",
    "        The indicators used for validating the measured symptoms and an exit criteria\n",
    "    \"\"\"\n",
    "    # presentedMeasuredSymptoms: list of measured and combination of measured symptoms\n",
    "    for index in range(0, len(presentedMeasuredSymptoms)):\n",
    "        #a dictionray list of indicators (used a gold standard for evalutation)\n",
    "        for j in range(0, len(indicators)):\n",
    "            # check exact matching, Exit Criteria\n",
    "            if dict(indicators[j], **presentedMeasuredSymptoms[index]) == indicators[j]:\n",
    "            #if presentedMeasuredSymptoms[index] == indicators[j]: \n",
    "                \n",
    "                #print(presentedMeasuredSymptoms[index])\n",
    "                temp = presentedMeasuredSymptoms[index]\n",
    "                \n",
    "                #Trace the evidence from hisotry for the presentedMeasuredSymptoms[index]\n",
    "            \n",
    "                #generatedResult.append(presentedMeasuredSymptoms[index])\n",
    "                result = pd.Series(data={'Measured_Symptoms':presentedMeasuredSymptoms[index],'Urgent_Attention':Urgent_Attention,'Generated_CP':CP,'Finding':finding, 'Evidence':'', 'Prior_Prob':'','Accuracy':'','Pred_CP':'','Severity':'', 'Cost':'', 'Weight':''}, name=len(Generated_CP_LIST))\n",
    "                Generated_CP_LIST = Generated_CP_LIST.append(result)\n",
    "    return Generated_CP_LIST  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinicalPathway_generator(Generated_CP_LIST, presentedMeasuredSymptoms, indicators['referralIndicators'][0], Urgent_Attention='Yes',CP='R',finding='Convulsion') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function for executing clinical pathways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_clincalPathways(Generated_CP_LIST):\n",
    "    #execute if there is any urgent conditions \n",
    "    Generated_CP_LIST = clinicalPathway_generator(Generated_CP_LIST, presentedMeasuredSymptoms, indicators['urgentIndicators'][0], Urgent_Attention='Yes',CP='NC',finding='UrgentAttention') \n",
    "\n",
    "    #execute referral clincal pathways\n",
    "    for i in range(0, len(indicators['referralIndicators'])):\n",
    "        if i == 0:\n",
    "            Generated_CP_LIST = clinicalPathway_generator(Generated_CP_LIST, presentedMeasuredSymptoms, indicators['referralIndicators'][i], Urgent_Attention='Yes',CP='R',finding='Convulsion') \n",
    "        elif i == 1:\n",
    "            Generated_CP_LIST = clinicalPathway_generator(Generated_CP_LIST, presentedMeasuredSymptoms, indicators['referralIndicators'][i], Urgent_Attention='Yes',CP='R',finding='severe_Pre_eclampsia')\n",
    "        elif i == 2:\n",
    "            Generated_CP_LIST = clinicalPathway_generator(Generated_CP_LIST, presentedMeasuredSymptoms, indicators['referralIndicators'][i], Urgent_Attention='Yes',CP='R',finding='severe_hypertension')\n",
    "        elif i == 3:\n",
    "            Generated_CP_LIST = clinicalPathway_generator(Generated_CP_LIST, presentedMeasuredSymptoms, indicators['referralIndicators'][i], Urgent_Attention='Yes',CP='R',finding='vaginalBleeding')\n",
    "        elif i == 4:\n",
    "            Generated_CP_LIST = clinicalPathway_generator(Generated_CP_LIST, presentedMeasuredSymptoms, indicators['referralIndicators'][i], Urgent_Attention='Yes',CP='R',finding='pretermLabour')\n",
    "        elif i == 5:\n",
    "            Generated_CP_LIST = clinicalPathway_generator(Generated_CP_LIST, presentedMeasuredSymptoms, indicators['referralIndicators'][i], Urgent_Attention='Yes',CP='R',finding='PROM')\n",
    "        else:\n",
    "            Generated_CP_LIST = clinicalPathway_generator(Generated_CP_LIST, presentedMeasuredSymptoms, indicators['referralIndicators'][i], Urgent_Attention='Yes',CP='R',finding='unsurePregnancy') \n",
    "            \n",
    "    #execute treatable clinical pathways\n",
    "    for t in range(0, len(indicators['treatableIndicators'])):\n",
    "        if t == 0:\n",
    "            Generated_CP_LIST = clinicalPathway_generator(Generated_CP_LIST, presentedMeasuredSymptoms, indicators['treatableIndicators'][t], Urgent_Attention='No',CP='T',finding='Convulsion') \n",
    "        elif t == 1:\n",
    "            Generated_CP_LIST = clinicalPathway_generator(Generated_CP_LIST, presentedMeasuredSymptoms, indicators['treatableIndicators'][t], Urgent_Attention='No',CP='T',finding='vaginalBleeding') \n",
    "        elif t == 2:\n",
    "            Generated_CP_LIST = clinicalPathway_generator(Generated_CP_LIST, presentedMeasuredSymptoms, indicators['treatableIndicators'][t], Urgent_Attention='No',CP='T',finding='pretermLabour') \n",
    "        #elif t == 3:\n",
    "        #   Finding='notUrgentAttention'\n",
    "        #   Generated_CP_LIST = cp_processingTest(Generated_CP_LIST, presentedMeasuredSymptoms, indicators['treatableIndicators'][t], Urgent_Attention='No',CP='T',finding=finding)  \n",
    "        else:\n",
    "            Generated_CP_LIST = clinicalPathway_generator(Generated_CP_LIST, presentedMeasuredSymptoms, indicators['treatableIndicators'][t], Urgent_Attention='No',CP='T',finding='unsurePregnancy') \n",
    "    \n",
    "    #execute conisderation clinical pathways i.e. multi-disease clincal pathways\n",
    "\n",
    "    return Generated_CP_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generated_CP_LIST = execute_clincalPathways(Generated_CP_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The generated CP output are: \")\n",
    "#Generated_CP_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generated_CP_LIST['Measured_Symptoms']\n",
    "Generated_CP_LIST.sort_values(by=['Generated_CP'],ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(Generated_CP_LIST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that converts the measured sysmptom dict into data frame format\n",
    "def Transform_DictValue_to_df(Generated_CP_LIST):\n",
    "    msdf = pd.DataFrame()\n",
    "    for i in range(0 , len(Generated_CP_LIST['Measured_Symptoms'])):\n",
    "        xx = pd.DataFrame([Generated_CP_LIST['Measured_Symptoms'][i]])\n",
    "        msdf = msdf.append(xx, 'sort=False')\n",
    "    return msdf\n",
    "\n",
    "def filiter_proceed_cpdf(Generated_CP_LIST):\n",
    "    cpdf = Generated_CP_LIST.filter(['Urgent_Attention','Generated_CP', 'Finding'], axis=1)\n",
    "    return cpdf \n",
    "    \n",
    "# merge the ms data frames and proceess data frames\n",
    "def mergeProceed_msDF_and_cpDF(msdf, cpdf):\n",
    "    concat_result = pd.concat([msdf, cpdf], sort=False, axis=1)\n",
    "    concat_result.fillna('', inplace=True)\n",
    "    return concat_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msdf = Transform_DictValue_to_df(Generated_CP_LIST)\n",
    "cpdf = filiter_proceed_cpdf(Generated_CP_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msdfCols = msdf.columns.tolist()\n",
    "msdfCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generatedCP_dataframe = mergeProceed_msDF_and_cpDF(msdf, cpdf)\n",
    "df1 = generatedCP_dataframe\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generated_CP_LIST.groupby('Generated_CP')['Finding'].value_counts().to_frame('Frequency')#summarised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reterive_unquie_CPs(df, cols):\n",
    "    cols = df.columns.tolist()\n",
    "    df1 = df\n",
    "    #df1['Generated_CP_Freq1'] = 1\n",
    "    df4 = df1.groupby(cols).Generated_CP_Freq1.count().reset_index()\n",
    "    return df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reterive_unquie_CPs(df1, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1\n",
    "cols = df2.columns.tolist()\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Generated_CP_Freq'] = 1\n",
    "df2 = df2.groupby(cols).Generated_CP_Freq.count().reset_index()\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df1), \"generated CPs\", len(df2),\"unique CPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Freq'] = 1\n",
    "df2.groupby(['Generated_CP','Finding', 'Urgent_Attention', 'BP']).Freq.count().reset_index()\n",
    "# df2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.crosstab(index=df1['Generated_CP'], columns=df1['Finding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=Generated_CP_LIST.reset_index()\n",
    "#df1['Freq']=df1.groupby(by='Generated_CP')['Generated_CP'].transform('count')\n",
    "#df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here is append\n",
    "for i in range(0 , len(Generated_CP_LIST['Measured_Symptoms'])):\n",
    "    xx = pd.DataFrame([Generated_CP_LIST['Measured_Symptoms'][i]])\n",
    "    msdf = msdf.append(xx, 'sort=True')\n",
    "msdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### here its merge\n",
    "cpdf = Generated_CP_LIST.filter(['Urgent_Attention','Generated_CP', 'Finding'], axis=1)\n",
    "cpdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_result = pd.concat([msdf, cpdf], sort=False, axis=1)\n",
    "concat_result.fillna('', inplace=True)\n",
    "concat_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = pd.DataFrame(list(Generated_CP_LIST['Measured_Symptoms'][27].items()),columns = ['MS','Value'])\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Generated_CP_LIST['Measured_Symptoms'][27].items()).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame()\n",
    "df1=df\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df, df1]\n",
    "result = pd.concat(frames)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(Generated_CP_LIST)):\n",
    "    df = pd.DataFrame(Generated_CP_LIST['Measured_Symptoms'][i].items()) \n",
    "    df1.append(df, ignore_index=True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracing Evidence\n",
    "\n",
    "The goal is to dynamically validate the above generated knowledge-based CP using evidence (data-driven) i.e. dynamically validate and map the knowledge based clinical pathways with local conditions or context or tracing the history. \n",
    "\n",
    "As per our meeting (at the mid of April). We were suggesting, two approaches for tracing evidence (or validating the knowledge based pathways).\n",
    "\n",
    "I.  During the clincal pathway generation (i.e. while executing clinicalPathway_generator function). This approaches is fine, if all the measured sysmptoms are available or presented. Otherwise, strategy II is recommended because there is a delay of measured sysmptoms result in real world implmentation.\n",
    "\n",
    "II. After finalizing the execution of knwledge based clincal pathways. This method is ideal and applicable after  executing the knowledge based pathways, to support and validate the decisions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcdata = pd.read_csv(\"Preg2020-Table 1_Updated.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check whether an evidence is found or not for the generated pathways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Copy the columns for the health center datasets\n",
    "pres = pd.DataFrame(columns=hcdata.columns)\n",
    "#drop the target class\n",
    "pres = pres.drop('CP', axis=1)\n",
    "pres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clinicalPathway_generator_MethodII(Generated_CP_LIST,presentedMeasuredSymptoms, indicators, Urgent_Attention, CP, finding):\n",
    "    \"\"\"\n",
    "        Generate the clinical pathway based on the measured symptoms\n",
    "        The indicators used for validating the measured symptoms and an exit criteria\n",
    "    \"\"\"\n",
    "    for index in range(0, len(presentedMeasuredSymptoms)):\n",
    "        for j in range(0, len(indicators)):\n",
    "            if presentedMeasuredSymptoms[index] == indicators[j]: # check exact matching, Exit Criteria\n",
    "                #print(presentedMeasuredSymptoms[index])\n",
    "                temp = presentedMeasuredSymptoms[index]\n",
    "                \n",
    "                #Trace the evidence from hisotry for the presentedMeasuredSymptoms[index]\n",
    "                if TracingCP.check_Evidence(temp) == 'Yes':\n",
    "                    Prob = TracingCP.trace_and_predict_ProbabiliticCP()\n",
    "                    Evidence = 'Yes'\n",
    "                else:\n",
    "                    Evidence = 'No'\n",
    "                    TracingCP.insert_unseen_measuredSymptoms(historicalRecords)\n",
    "                #generatedResult.append(presentedMeasuredSymptoms[index])\n",
    "                result = pd.Series(data={'Measured_Symptoms':presentedMeasuredSymptoms[index],'Urgent_Attention':Urgent_Attention,'CP':CP,'Finding':finding, 'Evidence':Evidence, 'Prob':Prob,'Severity':'', 'Cost':'', 'Weight':''}, name=len(Generated_CP_LIST))\n",
    "                Generated_CP_LIST = Generated_CP_LIST.append(result)\n",
    "    return Generated_CP_LIST  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Update_CPModel():\n",
    "    \"\"\"\n",
    "        Re-train and update the CP Model based on new records \n",
    "    \"\"\"\n",
    "    def splitTargetClass(data):\n",
    "        X = data.drop('CP', axis=1)\n",
    "        y = data['CP']\n",
    "        return X,y\n",
    "    \n",
    "    \n",
    "    def re_Train_CPModel(historicalRecords):\n",
    "        \"\"\"\n",
    "            SMOTE and 10 cross validation gain reasonbale preformance during experimentation \n",
    "        \"\"\"\n",
    "        \n",
    "        #Fill the missing value: not available ....\n",
    "        historicalRecords=TracingCP.fill_missing_values(historicalRecords)\n",
    "        \n",
    "        # encode the data\n",
    "        historicalRecords = historicalRecords.apply(LabelEncoder().fit_transform)\n",
    "        \n",
    "        # split the class\n",
    "        X,y = Update_CPModel.splitTargetClass(historicalRecords)\n",
    "        \n",
    "        #handle the data imbalnce with\n",
    "        os_us = SMOTETomek(sampling_strategy=0.5)\n",
    "        X_smote_res, y_smote_res = os_us.fit_sample(X, y)\n",
    "\n",
    "        print(\"10-Fold Cross-Validation: NB Accuracy with SMOTE  dataset\", cross_val_score(GaussianNB(), X_smote_res, y_smote_res, cv=10), \n",
    "                     \"\", \"Average\", np.average(cross_val_score(GaussianNB(), X_smote_res, y_smote_res, cv=10)))\n",
    "        \n",
    "        #Save the updated model\n",
    "        \"\"\"                \n",
    "        Update_CPModel.save_the_Updated_CPModel(CPModel)\n",
    "        \"\"\"\n",
    "    \n",
    "    def save_the_Updated_CPModel(*args):\n",
    "        # Save the Modle to file in the current working directory\n",
    "        Pkl_Filename = \"Pickle_CP_Model.pkl\"  \n",
    "\n",
    "        with open(Pkl_Filename, 'wb') as file:  \n",
    "            pickle.dump(CPModel, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TracingCP:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def check_Evidence(pres, *args):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        getColumnList=[]\n",
    "        \"\"\"\n",
    "            Retrieving the column name\n",
    "        \"\"\"\n",
    "        for col in Generated_CP_LIST['Measured_Symptoms'][i]:\n",
    "            getColumnList.append(col)\n",
    "        \"\"\"\n",
    "            Check the retrieved column found in the existing record or not\n",
    "        \"\"\"\n",
    "        if set(getColumnList).issubset(pres.columns):\n",
    "            Flag = \"Yes\"\n",
    "        else:\n",
    "            Flag = \"No\"\n",
    "        return Flag\n",
    "    \n",
    "\n",
    "    def load_clinicalPathways_model(*args):\n",
    "        \"\"\"\n",
    "            Load the clinical pathway model back from file. The model was trained and saved in the file. The model is\n",
    "            reterived for predciting and caculating the posterior probablity. \n",
    "        \"\"\"\n",
    "        Pkl_Filename = \"Pickle_CP_Model.pkl\"\n",
    "        if os.path.exists(Pkl_Filename):  \n",
    "            try: \n",
    "                with open(Pkl_Filename, 'rb') as file:  \n",
    "                    Pickled_CP_Model = pickle.load(file)\n",
    "                return Pickled_CP_Model\n",
    "            except EOFError:\n",
    "                return \"It's Empty Pickeled Model\"\n",
    "    \n",
    "    \n",
    "    def trace_and_predict_ProbabiliticCP(*args):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Call the function to fill the missing values \n",
    "        TracingCP.fill_missing_values(presented)\n",
    "\n",
    "        #Encode the presented symptoms for calculating the probability and the target\n",
    "        encoded_presented = presented.apply(LabelEncoder().fit_transform)\n",
    "        #print(encoded_presented)\n",
    "        \n",
    "        #get the clinical pathway model\n",
    "        ClinicalPathwayModel = TracingCP.load_clinicalPathways_model()\n",
    "        \n",
    "\n",
    "        #Predict using the \n",
    "        y_pred = ClinicalPathwayModel.predict(encoded_presented)\n",
    "        \n",
    "        #Return the mean accuracy on the given test data and labels.\n",
    "        #accuracy = ClinicalPathwayModel.score(X_test, y_test)\n",
    "        accuracy = ClinicalPathwayModel.score(encoded_presented, y_pred)\n",
    "\n",
    "        #get the priors\n",
    "        priorClass = ClinicalPathwayModel.class_prior_\n",
    "        \n",
    "        #predictive probablities \n",
    "        print(\"Pred_CP_Class:\",y_pred,\n",
    "              \"Pred_Prob:\", ClinicalPathwayModel.predict_proba(encoded_presented).mean(),\n",
    "              \"predict_log_proba:\", ClinicalPathwayModel.predict_log_proba(encoded_presented).mean(),\n",
    "              \"Accuracy:\",accuracy)\n",
    "        \n",
    "        return y_pred,priorClass,accuracy\n",
    "    \n",
    "    def insert_unseen_measuredSymptoms(historicalRecords,*args):\n",
    "        \"\"\"\n",
    "            This function aims to insert unseen measured symptoms into the healthcenter records.\n",
    "            Append on the historical records for future tracing\n",
    "        \"\"\"\n",
    "        historicalRecords = historicalRecords.append(Generated_CP_LIST['Measured_Symptoms'][i], ignore_index=True)\n",
    "        \n",
    "        #call, to update the save model using the new unseen records\n",
    "        Update_CPModel.re_Train_CPModel(historicalRecords)\n",
    "        \n",
    "        return historicalRecords.shape\n",
    "\n",
    "    def fill_missing_values(presented):\n",
    "        #Fill the missing values\n",
    "        for col in presented.columns:\n",
    "            # replacing na values in college with No college \n",
    "            presented[col].fillna(\"Notavailable\", inplace = True) \n",
    "        return presented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TracingCP.load_clinicalPathways_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Check wether an evidence is found or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(Generated_CP_LIST['Measured_Symptoms'])):\n",
    "    #print(Generated_CP_LIST['Measured_Symptoms'][i])\n",
    "    Evidence = TracingCP.check_Evidence(pres, Generated_CP_LIST['Measured_Symptoms'][i])\n",
    "    if Generated_CP_LIST['Evidence'][i] == '':\n",
    "        Generated_CP_LIST['Evidence'][i] = Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generated_CP_LIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracing Probablity for the Measured Symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "#encoded_data = hcdata.apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy the columns for the health center datasets\n",
    "presented = pd.DataFrame(columns=hcdata.columns)\n",
    "#drop the target class\n",
    "presented = presented.drop('CP', axis=1)\n",
    "presented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(Generated_CP_LIST['Measured_Symptoms'])):\n",
    "    if Generated_CP_LIST['Evidence'][i] == 'Yes':\n",
    "        #Make sure the new row is empty\n",
    "        presented = presented.iloc[0:0]\n",
    "        \n",
    "        #get the new measured symptoms \n",
    "        new_row = pd.Series(data=Generated_CP_LIST['Measured_Symptoms'][i])\n",
    "        \n",
    "        #append the measured symptoms and prepare the new rows for predictions \n",
    "        presented = presented.append(new_row,ignore_index=True)\n",
    "        presented.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "        #fill the missing values \n",
    "        presented = TracingCP.fill_missing_values(presented)\n",
    "        \n",
    "        # Predic the CP for the measured symptoms\n",
    "        pred_CP_Class, priorClass_Prob,accuracy = TracingCP.trace_and_predict_ProbabiliticCP(presented)\n",
    "        \n",
    "        #assign the new prediction class on the decision table \n",
    "        Generated_CP_LIST['Prior_Prob'][i] = priorClass_Prob.round(2)\n",
    "        \n",
    "        if pred_CP_Class == 1:\n",
    "            Generated_CP_LIST['Pred_CP'][i] = 'T'\n",
    "        else:\n",
    "            Generated_CP_LIST['Pred_CP'][i] = 'R'\n",
    "        \n",
    "        Generated_CP_LIST['Accuracy'][i] = accuracy\n",
    "    else:\n",
    "        # Evidence is not found, based on the measured Symptoms\n",
    "        # Add on the historical record; train and update the saved model\n",
    "        historicalRecords=hcdata\n",
    "        #TracingCP.insert_unseen_measuredSymptoms(historicalRecords)\n",
    "        #NewpresValue = Generated_CP_LIST['Measured_Symptoms'][i]\n",
    "        \n",
    "        #test = test.append(NewpresValue,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generated_CP_LIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CP Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    if IS Evidence(TRUE) then\n",
    "        Visualize the CP ranking based on Evidence ; Display the ranking difference, If there is any ;\n",
    "    else if IS choice Found(TRUE) then\n",
    "        Process the CP ranking based on the choice such as Probability, Severity, or Cost ;\n",
    "    else if IS manualWeight Found then\n",
    "        Visualize the CP ranking based on the manual weight refinement ;\n",
    "    else\n",
    "        Process accordingly (default processing);\n",
    "\"\"\"\n",
    "\n",
    "class clinicalPathways_Ranking:\n",
    "    #Generated_CP_LIST['Measured_Symptoms']\n",
    "    \n",
    "    def with_Evidence(Generated_CP_LIST, *args):\n",
    "        #Ranking based on Evidence\n",
    "        return Generated_CP_LIST[Generated_CP_LIST['Evidence']=='Yes']\n",
    "    \n",
    "    def without_Evidence(Generated_CP_LIST, *args):\n",
    "        #print(\"Generated Clinical Pathways with no Evidence\")\n",
    "        return Generated_CP_LIST[Generated_CP_LIST['Evidence'] == 'No']\n",
    "\n",
    "    def defaultRanking(Generated_CP_LIST, *args):\n",
    "        #Default processing: First, the Treatable CP. Second, Referral CP. Lastly, NC(Not Classified)\n",
    "        return Generated_CP_LIST.sort_values(by=['CP'],ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RankingwithEvidence = clinicalPathways_Ranking.with_Evidence(Generated_CP_LIST)\n",
    "RankingwithEvidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RankingwithoutEvidence = clinicalPathways_Ranking.without_Evidence(Generated_CP_LIST)\n",
    "RankingwithoutEvidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CP Prunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    if generated CP LIST Is EMPTY then\n",
    "        Go to Algorithm-5 and adjust the Criteria (fall back on CP ranking). Adjust and eliminate one ;\n",
    "    else if too much pruning (TRUE) then \n",
    "        Display eventual warning ;\n",
    "    else\n",
    "        Process the pruning based on the pruning parameters ;\n",
    "        If it FULFILL the endorsed indicator, EXIT;\n",
    "    end\n",
    "\"\"\"\n",
    "class clinicalPathways_Pruning:\n",
    "    def default(*args):\n",
    "        #the goal is to fliter out referral pathways quickly to minimize delay\n",
    "        return Generated_CP_LIST.loc[(Generated_CP_LIST['Evidence']>='Yes') & (Generated_CP_LIST['Generated_CP'] == 'R')]\n",
    "    \n",
    "    def using_Urgent_Attention(*args):\n",
    "        #Quickly identifies urgent attention based on CGs (Gold Standard)\n",
    "        try:\n",
    "            return Generated_CP_LIST[Generated_CP_LIST['Urgent_Attention'] == 'Yes']\n",
    "        except Exception as e:\n",
    "            print('There was an error in your urgent attention value, The value is empty :{0}'.format(e))\n",
    "    \n",
    "    def using_Evidence(*args):\n",
    "        try:\n",
    "            return Generated_CP_LIST[Generated_CP_LIST['Evidence'] == 'No']\n",
    "        except Exception as e:\n",
    "            print('There was an error in your evidence value, The value is empty :{0}'.format(e))\n",
    "    \n",
    "    def using_Prob(*args):\n",
    "        return Generated_CP_LIST[Generated_CP_LIST['Prob'] >= 0.5]\n",
    "    \n",
    "    def using_Severity(*args):\n",
    "        try:\n",
    "            return Generated_CP_LIST[Generated_CP_LIST['Severity'] >= 0.5]\n",
    "        except Exception as e:\n",
    "            print('There was an error in your severity value, The value is empty:{0}'.format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinicalPathways_Pruning.default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evd = Generated_CP_LIST.loc[(Generated_CP_LIST['Evidence']>='Yes') & (Generated_CP_LIST['Generated_CP'] == 'R')& (Generated_CP_LIST['Pred_CP'] == 'T')]\n",
    "evd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output: Match Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    The exact match between dynamically generated CP and data-driven CP and their variation.\n",
    "\"\"\"\n",
    "def macthAnalysis(output, KB_CP, DD_CP):\n",
    "    evdR = output.loc[(output['Evidence']>='Yes') & (output['Generated_CP'] == KB_CP )& (output['Pred_CP'] == DD_CP)] \n",
    "    return evdR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macthAnalysis(Generated_CP_LIST, \"R\",\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generated_CP_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Generated_CP_LIST['Measured_Symptoms'][7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(Generated_CP_LIST['Measured_Symptoms'])):\n",
    "    if Generated_CP_LIST['Evidence'][i] == 'Yes':\n",
    "        # print(Generated_CP_LIST['Measured_Symptoms'][i])\n",
    "        new_row = pd.Series(data=Generated_CP_LIST['Measured_Symptoms'][i])\n",
    "        presented = presented.append(new_row,ignore_index=True)\n",
    "        presented.reset_index(inplace=True, drop=True)\n",
    "        TracingCP.trace_and_predict_ProbabiliticCP(presented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generated_CP_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = pd.Series(data=Generated_CP_LIST['Measured_Symptoms'][4])\n",
    "presented = presented.append(new_row,ignore_index=True)\n",
    "presented.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TracingCP.trace_and_predict_ProbabiliticCP(presented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(Generated_CP_LIST['Measured_Symptoms'])):\n",
    "    #print(check_Column(Generated_CP_LIST['Measured_Symptoms'][i]))\n",
    "    Evidence = TracingCP.check_Evidence(pres, Generated_CP_LIST['Measured_Symptoms'][i])\n",
    "    Generated_CP_LIST['Evidence'][i] = Evidence\n",
    "    \"\"\"\n",
    "        If evidence is found in the historical record using the measured symptoms\n",
    "        Retrieve the pre-train clinical pathway model for prediction and tracing the probability for the presented measured symptoms.\n",
    "    \"\"\"\n",
    "    if Evidence == 'Yes' and Generated_CP_LIST['Prob'][i] == '': \n",
    "        Generated_CP_LIST['Prob'][i]=y_pred\n",
    "        #call a function for tracing the prepability \n",
    "    else:\n",
    "        \"\"\"\n",
    "            Add the measured symptoms on the existing dataset\n",
    "            Train and update the Clinical Pathway Model \n",
    "            Update the the Clinical Pathway Model\n",
    "        \"\"\"\n",
    "        #call the function for adding the unseen records\n",
    "        pres = TracingCP.insert_unseen_measuredSymptoms(pres, Generated_CP_LIST['Measured_Symptoms'][i])\n",
    "        \n",
    "        #pres = pres.append(Generated_CP_LIST['Measured_Symptoms'][i], ignore_index=True)\n",
    "        # re-call the clinical pathway model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pres = TracingCP.fill_missing_values(pres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pres = pres.iloc[0:0]\n",
    "pres= pres.drop(['convulsion', 'blurredVision'], axis=1)\n",
    "pres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre= pre.drop(['convulsion', 'blurredVision'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre[0:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generated_CP_LIST['Measured_Symptoms'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dyna_join(df, positions):\n",
    "    return pd.concat([df, df.iloc[:, positions].apply(','.join, 1).rename('new_col')], axis=1)\n",
    "dyna_join(pre, [0, -2])\n",
    "pre[0:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(Generated_CP_LIST['Measured_Symptoms'])):\n",
    "    #print(Generated_CP_LIST['Measured_Symptoms'][i])\n",
    "    Evidence = check_Evidence(Generated_CP_LIST['Measured_Symptoms'][i])\n",
    "    if Evidence=='No':\n",
    "        \"\"\"\n",
    "            #ADD: if the measured symptoms is not avilable; \n",
    "            #Append on the historical records for future tracing\n",
    "        \"\"\"\n",
    "        pre = pre.append(Generated_CP_LIST['Measured_Symptoms'][i], ignore_index=True)\n",
    "        \"\"\"\n",
    "            Fill a value of not available ....\n",
    "        \"\"\"\n",
    "        fill_missing_values(pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = insert_unseen_measuredSymptoms(hcdata, ms)\n",
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"    \n",
    "    If the measured symptoms are found in the historical record: \n",
    "        Retrieve the measured sysmptoms from Generated_CP_LIST \n",
    "        Call and retrieve the pre-train clinical pathway model for prediction and tracingthe probability.\n",
    "    else\n",
    "        Add the measured symptoms in the historical record as new information \n",
    "        Train and update the Clinical Pathway Model \n",
    "        Save the Clinical Pathway Model\n",
    "\"\"\"\n",
    "\n",
    "# If all the columns are avilable\n",
    "pre = pre.append(Generated_CP_LIST['Measured_Symptoms'][1], ignore_index=True)\n",
    "pre['BP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generated_CP_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(presented):\n",
    "    #Fill the missing values\n",
    "    for col in presented.columns:\n",
    "        # replacing na values in college with No college \n",
    "        presented[col].fillna(\"Notavailable\", inplace = True) \n",
    "    return presented[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_missing_values(pre)\n",
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(columns=hcdata.columns)\n",
    "\n",
    "Generated_CP_LIST['Measured_Symptoms'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADD: if the column is not avilable; append the record on the historical records\n",
    "test = test.append(Generated_CP_LIST['Measured_Symptoms'][0], ignore_index=True)\n",
    "fill_missing_values(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClinicalPathwayModel = load_clinicalPathways_model()\n",
    "#ClinicalPathwayModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete all rows in pandas\n",
    "pre=pre[0:0]\n",
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the presented symptoms for calculating the probability and the target\n",
    "encoded_presented = pre.apply(LabelEncoder().fit_transform)\n",
    "#print(encoded_presented)\n",
    "\n",
    "#Predict using the \n",
    "y_pred = ClinicalPathwayModel.predict(encoded_presented)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_Evidence(*args):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    getColumnList=[]\n",
    "    \"\"\"\n",
    "        Retrieving the column name\n",
    "    \"\"\"\n",
    "    for col in Generated_CP_LIST['Measured_Symptoms'][i]:\n",
    "        getColumnList.append(col)\n",
    "    \"\"\"\n",
    "        Check the retrieved column found in the existing record or not\n",
    "    \"\"\"\n",
    "    if set(getColumnList).issubset(pres.columns):\n",
    "        Flag = \"Yes\"\n",
    "    else:\n",
    "        Flag = \"No\"\n",
    "    return Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function for checking evidence \n",
    "def getEvidence(hcdata,measuredSymptoms, Evidence='No'):\n",
    "    \"\"\"\n",
    "        This function checks whether  an evidence or not \n",
    "    \"\"\"\n",
    "    if len(measuredSymptoms)>1:\n",
    "        for key, val in measuredSymptoms.items():\n",
    "            if key in hcdata.columns:\n",
    "                Evidence='Yes'\n",
    "    else:\n",
    "        key, val = next(iter(measuredSymptoms.items())) \n",
    "        if key in hcdata.columns:\n",
    "            Evidence='Yes'\n",
    "    return key, val, Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in range(0, len(Generated_CP_LIST['Measured_Symptoms'])):\n",
    "    key, val, Evidence= getEvidence(hcdata, Generated_CP_LIST['Measured_Symptoms'][col])\n",
    "    Generated_CP_LIST['Evidence'][col] = Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Result\n",
    "Generated_CP_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select those only have evidence\n",
    "#Generated_CP_LIST[Generated_CP_LIST['Evidence']=='Yes']\n",
    "\n",
    "#Generated_CP_LIST[Generated_CP_LIST['Evidence'] == 'No']\n",
    "hcdata.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(presented):\n",
    "    #Fill the missing values\n",
    "    for col in presented.columns:\n",
    "        # replacing na values in college with No college \n",
    "        presented[col].fillna(\"Notavailable\", inplace = True) \n",
    "    return presented[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy the columns for the health center datasets\n",
    "pre = pd.DataFrame(columns=hcdata.columns)\n",
    "#drop the target class\n",
    "pre = pre.drop('CP', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete all rows in pandas\n",
    "pre=pre[0:0]\n",
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set(getColumnList).issubset(pre.columns):\n",
    "    print(\"Yes\")\n",
    "else:\n",
    "    print(\"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in temp:\n",
    "    #print(col, temp[col])\n",
    "    if col in pre:\n",
    "        Flag = False\n",
    "    else:\n",
    "        Flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check whether the given column is found the historical record or not\n",
    "Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if all the columns are avilable\n",
    "pre = pre.append(temp, ignore_index=True)\n",
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = Generated_CP_LIST['Measured_Symptoms'][5]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in Generated_CP_LIST['Measured_Symptoms']:\n",
    "    pre = pre.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pres = pres.append(temp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting into list of tuple \n",
    "#list = [(k, v) for k, v in temp.items()] \n",
    "for i in temp:\n",
    "    print(i, temp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Generated_CP_LIST['Measured_Symptoms'][5]:\n",
    "    print(i, Generated_CP_LIST['Measured_Symptoms'][5][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pres\n",
    "for col in Generated_CP_LIST['Measured_Symptoms']:\n",
    "    #print(col)\n",
    "    for i in col:\n",
    "        #print(i, col[i])\n",
    "        if i in pres.columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = pd.Series(data=Generated_CP_LIST['Measured_Symptoms'][4])\n",
    "presented = presented.append(new_row,ignore_index=True)\n",
    "presented.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Call the function to fill the missing values \n",
    "fill_missing_values(presented)\n",
    "\n",
    "#Encode the presented symptoms for calculating the probability and the target\n",
    "encoded_presented = presented.apply(LabelEncoder().fit_transform)\n",
    "#print(encoded_presented)\n",
    "\n",
    "#Predict using the \n",
    "y_pred = ClinicalPathwayModel.predict(encoded_presented)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measuredSymptoms = Generated_CP_LIST['Measured_Symptoms'][5]\n",
    "#print(len(measuredSymptoms))\n",
    "#new_row = pd.Series(measuredSymptoms)\n",
    "#print(new_row)\n",
    "#new_row['headache'], new_row['BP']\n",
    "temp = measuredSymptoms.keys()\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy the columns for the health center datasets\n",
    "pres = pd.DataFrame(columns=hcdata.columns)\n",
    "#drop the target class\n",
    "pres = pres.drop('CP', axis=1)\n",
    "pres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in range (0, len(Generated_CP_LIST['Measured_Symptoms'])):\n",
    "    # print(Generated_CP_LIST['Measured_Symptoms'][col])\n",
    "    temp = pd.Series(Generated_CP_LIST['Measured_Symptoms'][col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe for building the model \n",
    "for col in range (0, len(Generated_CP_LIST['Measured_Symptoms'])):\n",
    "    new_row = pd.Series(data=Generated_CP_LIST['Measured_Symptoms'][col])\n",
    "    print(new_row)\n",
    "    presented = presented.append(new_row,ignore_index=True)\n",
    "    presented.reset_index(inplace=True, drop=True)\n",
    "    print(new_row)\n",
    "    # Call the function to fill the missing values \n",
    "    fill_missing_values(presented)\n",
    "    #Print(presented)\n",
    "    \n",
    "    #Encode the presented symptoms for calculating the probability and the target\n",
    "    encoded_presented = presented.apply(LabelEncoder().fit_transform)\n",
    "    #print(encoded_presented)\n",
    "    \n",
    "    #predict the target class\n",
    "    y_pred = CPmodel.predict(encoded_presented)\n",
    "    y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class traceEvidence1:\n",
    "    #Probabilistic Evidence Table \n",
    "    def calculate_Probability(key):\n",
    "        \"\"\"\n",
    "            Calculating the probablity (referral or treated) based on historical evidence\n",
    "        \"\"\"\n",
    "        df_s = hcdata.groupby(key)['CP'].value_counts() / hcdata.groupby(key)['CP'].count()\n",
    "        df_f = df_s.reset_index(name='Probability')\n",
    "        probEvidenceTable = df_f[df_f.values == 'Yes']\n",
    "        return probEvidenceTable\n",
    "    \n",
    "    #get the calculated probablity\n",
    "    def get_R_Probability(key):\n",
    "        probEvidenceTable = traceEvidence1.calculate_Probability(key)\n",
    "        if 'Refer' in probEvidenceTable.CP.values:\n",
    "            prob = probEvidenceTable[probEvidenceTable['CP'] == 'Refer'][['Probability']].values\n",
    "        else: \n",
    "            prob = 0\n",
    "        return prob\n",
    "    def get_T_Probability(key):\n",
    "        probEvidenceTable = traceEvidence1.calculate_Probability(key)\n",
    "        if ['Treated' in probEvidenceTable.CP.values]:\n",
    "            prob = probEvidenceTable[probEvidenceTable['CP'] == 'Treated'][['Probability']].values\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in range(0, len(Generated_CP_LIST['Measured_Symptoms'])):\n",
    "    key, val, Evidence= getEvidence(hcdata, Generated_CP_LIST['Measured_Symptoms'][col])\n",
    "    print(key)\n",
    "    if key in hcdata.columns:\n",
    "        if Generated_CP_LIST['CP'][col] == 'R':\n",
    "            prob = traceEvidence1.get_R_Probability(key)\n",
    "        if Generated_CP_LIST['CP'][col] == 'T':\n",
    "            prob = traceEvidence1.get_T_Probability(key)\n",
    "    else:\n",
    "        prob=0\n",
    "    #Generated_CP_LIST['Prob'][col] = prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#traceEvidence1.get_R_Probability('headache')\n",
    "key = 'headache'\n",
    "#hcdata.groupby(key)['CP'].value_counts() / hcdata.groupby(key)['CP'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#problem where there is a logical and\n",
    "Generated_CP_LIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for col in range (0, len(Generated_CP_LIST['Measured_Symptoms'])):\n",
    "    presented = pd.DataFrame(columns=hcdata.columns)\n",
    "    new_row = pd.Series(data=presentedMeasuredSymptoms[4])\n",
    "    presented = presented.append(new_row,ignore_index=True)\n",
    "    presented.reset_index(inplace=True, drop=True)\n",
    "    presented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracing the evidence based on the measured symptoms  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hcdata = pd.read_csv(\"Preg2020-Table 1.csv\", encoding='utf-8')\n",
    "hcdata = pd.read_csv(\"Preg2020-Table 1_Updated.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by CP and summarize disease name \n",
    "hcdata.groupby([\"CP\"])[[\"CP\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column\n",
    "def missing_values_table(df):\n",
    "   \n",
    "    # Total missing values\n",
    "    mis_val = df.isnull().sum()\n",
    "    \n",
    "    # Percentage of missing values\n",
    "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "    \n",
    "    # Make a table with the results\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "    \n",
    "    # Rename the columns\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(\n",
    "    columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "    \n",
    "    # Sort the table by percentage of missing descending\n",
    "    # .iloc[:, 1]!= 0: filter on missing missing values not equal to zero\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "        mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "    '% of Total Values', ascending=False).round(2)  # round(2), keep 2 digits\n",
    "    \n",
    "    # Print some summary information\n",
    "    print(\"Your slelected dataframe has {} columns.\".format(df.shape[1]) + '\\n' + \n",
    "    \"There are {} columns that have missing values.\".format(mis_val_table_ren_columns.shape[0]))\n",
    "    \n",
    "    # Return the dataframe with missing information\n",
    "    return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_table(hcdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(presented):\n",
    "    #Fill the missing values\n",
    "    for col in presented.columns:\n",
    "        # replacing na values in college with No college \n",
    "        presented[col].fillna(\"Notavailable\", inplace = True) \n",
    "    return presented[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_missing_values(hcdata)\n",
    "missing_values_table(hcdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old data \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoded_data = hcdata.apply(LabelEncoder().fit_transform)\n",
    "encoded_data.head(2)\n",
    "#len(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTargetClass(data):\n",
    "    X = data.drop('CP', axis=1)\n",
    "    y = data['CP']\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=splitTargetClass(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=42)\n",
    "CPmodel = GaussianNB()\n",
    "CPmodel.fit(X_train,y_train)\n",
    "y_pred = CPmodel.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import pickle Package\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Modle to file in the current working directory\n",
    "Pkl_Filename = \"Pickle_CP_Model.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(CPmodel, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Model back from file\n",
    "with open(Pkl_Filename, 'rb') as file:  \n",
    "    Pickled_CP_Model = pickle.load(file)\n",
    "\n",
    "Pickled_CP_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracing the evidence based on the new measured symptoms for predicting CP class (Referral or Treated) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(presented):\n",
    "    #Fill the missing values\n",
    "    for col in presented.columns:\n",
    "        # replacing na values in college with No college \n",
    "        presented[col].fillna(\"Notavailable\", inplace = True) \n",
    "    return presented[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy the columns for the health center datasets\n",
    "presented = pd.DataFrame(columns=hcdata.columns)\n",
    "#drop the target class\n",
    "presented = presented.drop('CP', axis=1)\n",
    "presented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = dict(Generated_CP_LIST['Measured_Symptoms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generated_CP_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating evidence for the presented measured symptoms\n",
    "Generated_CP_LIST['Measured_Symptoms'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generated_CP_LIST['Measured_Symptoms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkEvidence(measuredSymptoms, Evidence='No'):\n",
    "    \"\"\"\n",
    "        This function checks whether there is an evidence or not for the presented symptoms\n",
    "    \"\"\"\n",
    "    if len(measuredSymptoms)>1:\n",
    "        for key, val in measuredSymptoms.items():\n",
    "            if key in hcdata.columns:\n",
    "                Evidence='Yes'\n",
    "    else:\n",
    "        key, val = next(iter(measuredSymptoms.items())) \n",
    "        if key in hcdata.columns:\n",
    "            Evidence='Yes'\n",
    "    return key, val, Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, val, evidence = checkEvidence(Generated_CP_LIST['Measured_Symptoms'][4], Evidence='No')\n",
    "key, val, evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcdata.groupby(key).size().div(len(hcdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s = hcdata.groupby(key)['CP'].value_counts() / hcdata.groupby(key)['CP'].count()\n",
    "df_f = df_s.reset_index(name='Probability')\n",
    "probEvidence = df_f[df_f.values == 'Yes']\n",
    "probEvidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probEvidence['abdominalPain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probEvidence.get_value(5, 'CP')\n",
    "treatedPro = probEvidence[probEvidence['CP'] == 'Treated'][['Probability']].values\n",
    "referralPro = probEvidence[probEvidence['CP'] == 'Refer'][['Probability']].values\n",
    "\n",
    "treatedPro,referralPro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcdata.groupby(key)['CP'].value_counts() / hcdata.groupby(key)['CP'].count()\n",
    "df_s.reset_index(name='Probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, val # Explore the referral and treatable evidence [Prior probability]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class traceEvidence:\n",
    "    #Probabilistic Evidence Table \n",
    "    def calculate_Probability(key):\n",
    "        \"\"\"\n",
    "            Calculating the probablity (referral or treated) based on historical evidence\n",
    "        \"\"\"\n",
    "        df_s = hcdata.groupby(key)['CP'].value_counts() / hcdata.groupby(key)['CP'].count()\n",
    "        df_f = df_s.reset_index(name='Probability')\n",
    "        probEvidenceTable = df_f[df_f.values == 'Yes']\n",
    "        return probEvidenceTable\n",
    "    \n",
    "    #get the calculated probablity\n",
    "    def get_Probability(*args):\n",
    "        probEvidenceTable = traceEvidence.calculate_Probability(key)\n",
    "        if 'Treated' in probEvidence.CP.values:\n",
    "            treatedPro = probEvidence[probEvidence['CP'] == 'Treated'][['Probability']].values\n",
    "        if 'Refer' in probEvidence.CP.values:\n",
    "            referralPro = probEvidence[probEvidence['CP'] == 'Refer'][['Probability']].values\n",
    "        return referralPro, treatedPro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "referralPro, treatedPro = traceEvidence.get_Probability(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "referralPro, treatedPro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Treated' in probEvidence.CP.values:\n",
    "    treatedPro = probEvidence[probEvidence['CP'] == 'Treated'][['Probability']].values\n",
    "    print(treatedPro) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Refer' in probEvidence.CP.values:\n",
    "    treatedPro = probEvidence[probEvidence['CP'] == 'Refer'][['Probability']].values\n",
    "    print(treatedPro) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatedPro = probEvidence[probEvidence['CP'] == 'Treated'][['Probability']].values\n",
    "treatedPro "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "referralPro = probEvidence[probEvidence['CP'] == 'Refer'][['Probability']].values\n",
    "referralPro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specific_ClinicalPathways(measuredSymptoms,*args):\n",
    "    #execute speicifc referral or treatable pathways based on the measured sysmptoms\n",
    "    ans = \"No path is found based on the measured symptoms\"\n",
    "    treatedDF = Generated_CP_LIST[(Generated_CP_LIST['Measured_Symptoms']==measuredSymptoms) & \n",
    "                        (Generated_CP_LIST['CP'] == 'T') ]\n",
    "    refferalDF = Generated_CP_LIST[(Generated_CP_LIST['Measured_Symptoms']==measuredSymptoms) & \n",
    "                        (Generated_CP_LIST['CP'] == 'R') ]\n",
    "    if len(treatedDF) or len(refferalDF) == 0:\n",
    "        return measuredSymptoms, ans\n",
    "    else:\n",
    "        return treatedDF, referralDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the measured sysmptoms, the value and availablity of evidence\n",
    "key, val, evidence = checkEvidence(vv)\n",
    "key, val, evidence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior probablity for the measured signs and symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(0, len(Generated_CP_LIST['Measured_Symptoms'])):\n",
    "    key, val, evidence = checkEvidence(Generated_CP_LIST['Measured_Symptoms'][k])\n",
    "    if evidence == 'Yes':\n",
    "        print(key, hcdata.groupby(key).size().div(len(hcdata)))\n",
    "    else:\n",
    "        print(key, \"has no evidence in the given dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Def a function to iterate and return the measured symptoms \n",
    "def availableEvidence(measuredSymptoms):\n",
    "    #if it's nested\n",
    "    if len(measuredSymptoms)>1:\n",
    "        for key, val in measuredSymptoms.items():\n",
    "            return key\n",
    "    else:\n",
    "        key, val = next(iter(measuredSymptoms.items())) \n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(0, len(Generated_CP_LIST['Measured_Symptoms'])):\n",
    "    ms = availableEvidence(Generated_CP_LIST['Measured_Symptoms'][k])\n",
    "    if ms in hcdata.columns:\n",
    "        print(ms,\"Yes\")\n",
    "    else:\n",
    "        print(ms, \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "availableEvidence(Generated_CP_LIST['Measured_Symptoms'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(Generated_CP_LIST['Measured_Symptoms'][5])>1:\n",
    "    temp = dict(Generated_CP_LIST['Measured_Symptoms'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evidence(measuredSymptoms):\n",
    "    if measuredSymptoms in hcdata.columns:\n",
    "        print(\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evidence(Generated_CP_LIST['Measured_Symptoms'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Sample Test for indvidual signs and symptoms\n",
    "new_row = pd.Series(data=Generated_CP_LIST['Measured_Symptoms'][4])\n",
    "presented = presented.append(new_row,ignore_index=True)\n",
    "presented.reset_index(inplace=True, drop=True)\n",
    "# Call the function to fill the missing values \n",
    "fill_missing_values(presented)\n",
    "#Encode the presented symptoms for calculating the probability and the target\n",
    "encoded_presented = presented.apply(LabelEncoder().fit_transform)\n",
    "#print(encoded_presented)\n",
    "\n",
    "#Predict using the \n",
    "y_pred = Pickled_CP_Model.predict(encoded_presented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in range (0, len(Generated_CP_LIST['Measured_Symptoms'])):\n",
    "    new_row = pd.Series(data=Generated_CP_LIST['Measured_Symptoms'][col])\n",
    "    presented = presented.append(new_row,ignore_index=True)\n",
    "    presented.reset_index(inplace=True, drop=True)\n",
    "    # Call the function to fill the missing values \n",
    "    fill_missing_values(presented)\n",
    "    #Encode the presented symptoms for calculating the probability and the target\n",
    "    encoded_presented = presented.apply(LabelEncoder().fit_transform)\n",
    "    #print(encoded_presented)\n",
    "    #y_pred = CPmodel.predict(encoded_presented)\n",
    "encoded_presented.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe for building the model \n",
    "for col in range (0, len(Generated_CP_LIST['Measured_Symptoms'])):\n",
    "    new_row = pd.Series(data=Generated_CP_LIST['Measured_Symptoms'][col])\n",
    "    print(new_row)\n",
    "    presented = presented.append(new_row,ignore_index=True)\n",
    "    presented.reset_index(inplace=True, drop=True)\n",
    "    print(new_row)\n",
    "    # Call the function to fill the missing values \n",
    "    fill_missing_values(presented)\n",
    "    #Print(presented)\n",
    "    \n",
    "    #Encode the presented symptoms for calculating the probability and the target\n",
    "    encoded_presented = presented.apply(LabelEncoder().fit_transform)\n",
    "    #print(encoded_presented)\n",
    "    \n",
    "    #predict the target class\n",
    "    y_pred = CPmodel.predict(encoded_presented)\n",
    "    y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new dataset for predicting the inputs\n",
    "presented = pd.DataFrame(columns=hcdata.columns)\n",
    "new_row = pd.Series(data={'Category':'Pregnancy', 'Type':'1rst ANC Visit', 'Status':'<16 weeks gestation','Headache':'Yes'}, name='x')\n",
    "presented = presented.append(new_row)\n",
    "presented.reset_index(inplace=True, drop=True)\n",
    "presented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presented = pd.DataFrame(columns=hcdata.columns)\n",
    "new_row = pd.Series(data=presentedMeasuredSymptoms[4])\n",
    "presented = presented.append(new_row,ignore_index=True)\n",
    "presented.reset_index(inplace=True, drop=True)\n",
    "presented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill the missing values\n",
    "for col in presented.columns:\n",
    "    # replacing na values in college with No college \n",
    "    presented[col].fillna(\"Notavailable\", inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presented = presented.drop('CP', axis=1)\n",
    "presented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_presented = presented.apply(LabelEncoder().fit_transform)\n",
    "encoded_presented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = CPmodel.predict(encoded_presented)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column\n",
    "def missing_values_table(df):\n",
    "   \n",
    "    # Total missing values\n",
    "    mis_val = df.isnull().sum()\n",
    "    \n",
    "    # Percentage of missing values\n",
    "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "    \n",
    "    # Make a table with the results\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "    \n",
    "    # Rename the columns\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(\n",
    "    columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "    \n",
    "    # Sort the table by percentage of missing descending\n",
    "    # .iloc[:, 1]!= 0: filter on missing missing values not equal to zero\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "        mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "    '% of Total Values', ascending=False).round(2)  # round(2), keep 2 digits\n",
    "    \n",
    "    # Print some summary information\n",
    "    print(\"Your slelected dataframe has {} columns.\".format(df.shape[1]) + '\\n' + \n",
    "    \"There are {} columns that have missing values.\".format(mis_val_table_ren_columns.shape[0]))\n",
    "    \n",
    "    # Return the dataframe with missing information\n",
    "    return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_table(hcdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcdata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presentedSymptomPro = hcdata.groupby('abdominalPain').size().div(len(hcdata))\n",
    "presentedSymptomPro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = hcdata.groupby(['CP','abdominalPain']).size().div(len(hcdata)).div(presentedSymptomPro,axis=0,level='abdominalPain')\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcdata.groupby('abdominalPain').count()['CP'] / len(hcdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(hcdata.groupby(['abdominalPain', 'CP']).count() / hcdata.groupby('abdominalPain').count())['Category']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s = hcdata.groupby('abdominalPain')['CP'].value_counts() / hcdata.groupby('abdominalPain')['CP'].count()\n",
    "df_f = df_s.reset_index(name='Probability')\n",
    "probEvidence = df_f[df_f.values == 'Yes']\n",
    "#df_f.head()  # your conditional probability table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probEvidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_Probability(hcdata, measuredSymptoms, CP):\n",
    "        # Calculating the probability from historical records\n",
    "        probDF = hcdata.groupby(measuredSymptoms)[CP].value_counts() / hcdata.groupby(measuredSymptoms)[CP].count()\n",
    "        #creating a proability column\n",
    "        probDF = probDF.reset_index(name='Probability')\n",
    "        \n",
    "        #select and return the measured sysmptoms  probabilistic evidences only\n",
    "        probEvidence = probDF[probDF.values == 'Yes']\n",
    "        return probEvidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probEvidence = calculate_Probability(hcdata, 'abdominalPain', 'CP')\n",
    "probEvidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "referalProb = probEvidence[probEvidence.values == 'Refer']['Probability']\n",
    "referalProb = referalProb.values[0]\n",
    "referalProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatedProb = probEvidence[probEvidence.values == 'Treated']['Probability']\n",
    "referalProb = treatedProb.values[0]\n",
    "referalProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"cp_means = hcdata.groupby('CP').count().mean() #var()\n",
    "cp_means\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,Class=splitTargetClass(hcdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(Class|HCData)=P(HCData|Class)*P(Class)/P(HCData)\n",
    "\n",
    "where:\n",
    "1. Class is a particular CP class (e.g. refer or treated)\n",
    "2. HCData is the health center data and features\n",
    "3. p(class∣HCData)is called the posterior\n",
    "4. p(HCData|class) is called the likelihood\n",
    "5. p(Class)is called the prior\n",
    "6. p(HCData) is called the marginal probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the Prior\n",
    "Prior = P(CP) i.e. how many times (Referral or Treated) appears/ total observations\n",
    "\n",
    "P(CP= Referral) P(CP=Treated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def priorProbability(data, Class):\n",
    "    prior = data.groupby(Class).size().div(len(data)) \n",
    "    return prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = priorProbability(hcdata,Class)\n",
    "prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Likelihood: \n",
    "Likelihood is generated for each of the features of the health center dataset. THe likelihood is probability of finding each feature given CP class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(data, Class):\n",
    "    for col in data.columns: \n",
    "        likelihood = {}\n",
    "        likelihood[col] = data.groupby([Class, col]).size().div(len(data)).div(prior)\n",
    "        #print(likelihood)\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = likelihood(data, Class)\n",
    "likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posteriorProbablity():\n",
    "    # Probability that the person will refer to the nearest \n",
    "    p_referral = likelihood['Age']['yes']['<=30'] * likelihood['Income']['yes']['medium'] * \\\n",
    "            likelihood['Student']['yes']['yes'] * likelihood['Credit_Rating']['yes']['fair'] \\\n",
    "            * prior['yes']\n",
    "\n",
    "    # Probability that the person will treated in the health center \n",
    "    p_no = likelihood['Age']['no']['<=30'] * likelihood['Income']['no']['medium'] * \\\n",
    "           likelihood['Student']['no']['yes'] * likelihood['Credit_Rating']['no']['fair'] \\\n",
    "           * prior['no']\n",
    "\n",
    "    print ('Yes : ', p_yes)\n",
    "    print ('No :  ', p_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = hcdata.groupby('CP').size().div(len(hcdata)) \n",
    "prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood['abdominalPain'] = hcdata.groupby(['CP', 'abdominalPain']).size().div(len(hcdata)).div(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood\n",
    "# Headache == 'Yes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(Class|features)=P(features|class)P(class)/P(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(Class|HCData)=P(HCData|Class)*P(Class)/P(HCData)\n",
    "\n",
    "where:\n",
    "1. Class is a particular CP class (e.g. refer or treated)\n",
    "2. HCData is the health center data and features\n",
    "3. p(class∣HCData)is called the posterior\n",
    "4. p(HCData|class) is called the likelihood\n",
    "5. p(Class)is called the prior\n",
    "6. p(HCData)is called the marginal probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Referral\n",
    "n_referral = hcdata['CP'][hcdata['CP'] == 'Refer'].count()\n",
    "\n",
    "# Number of Treated\n",
    "n_treated = hcdata['CP'][hcdata['CP'] == 'Treated'].count()\n",
    "\n",
    "# Total rows\n",
    "total_ppl = hcdata['CP'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of referral divided by the total rows\n",
    "P_referral = n_referral/total_ppl\n",
    "\n",
    "# Number of treated divided by the total rows\n",
    "P_treated = n_treated/total_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_referral, P_treated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by CP and calculate the means of each feature\n",
    "hcdata_means = hcdata.groupby('CP').count().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by CP and calculate the variance of each feature\n",
    "hcdata_variance = hcdata.groupby('CP').count().var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Means for Treated\n",
    "# feature_mean = hcdata_means['AbdominalPain'][hcdata_variance.index == 'Treated'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcdata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoded_data = hcdata.apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(encoded_data.drop(['CP'], axis=1), encoded_data['CP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the dataset in train test using scikit learn\n",
    "# now the model will train in training dataset and then we will use test dataset to predict its accuracy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now preparing our model as per Gaussian Naive Bayesian\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB().fit(X_train, y_train) #fitting our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = model.predict(X_test) #now predicting our model to our test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# now calculating that how much accurate our model is with comparing our predicted values and y_test values\n",
    "accuracy_score = accuracy_score(y_test, predicted_y) \n",
    "print (accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NewPerson = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NewPerson['Headache'] = ['Yes']\n",
    "NewPerson['Category'] = ['Test']\n",
    "NewPerson\n",
    "\n",
    "NewPerson_data = NewPerson.apply(LabelEncoder().fit_transform)\n",
    "NewPerson_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data is stored in Datadrame person\n",
    "predicted_y = model.predict(NewPerson_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
